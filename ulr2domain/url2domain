#!/usr/bin/env python
"""
Convert URLs to domains


  echo -e "dfhgfdh\nhttp://samoanic.ws/index.dhtml sdfg  sdfsf dfgdfgs.com http:// docs.python.org/" | ./url2domain -x -q
    samoanic.ws
    dfgdfgs.com
    python.org


"""
import sys
import fileinput
from optparse import OptionParser

# Get here:
# http://mxr.mozilla.org/mozilla/source/netwerk/dns/src/effective_tld_names.dat?raw=1
EFFECTIVE_TLD = "effective_tld_names.dat"


def get_tlds():
    raw = open(EFFECTIVE_TLD).readlines()
    # remove comments, emptly lines and strip
    return [line.strip() for line in raw if line[0] not in "/\n"]


def get_domain(url, tlds):
    # Credits to http://stackoverflow.com/questions/1066933/how-to-extract-domain-name-from-url
    # from urlparse import urlparse
    # url_elements = urlparse(url)[1].split('.')

    # No, we use simpler split since we might have links without 'http://'
    url_elements = url.split('.')
    if len(url_elements) < 2:
        raise ValueError("Domain candidate is too short")
    # print "\t", url, url_elements
    # url_elements = ["abcde","co","uk"]

    for i in range(-len(url_elements), 0):
        last_i_elements = url_elements[i:]
        #    i=-3: ["abcde","co","uk"]
        #    i=-2: ["co","uk"]
        #    i=-1: ["uk"] etc

        candidate = ".".join(last_i_elements)  # abcde.co.uk, co.uk, uk
        wildcard_candidate = ".".join(["*"] + last_i_elements[1:])  # *.co.uk, *.uk, *
        exception_candidate = "!" + candidate

        # match tlds:
        if (exception_candidate in tlds):
            return ".".join(url_elements[i:])
        if (candidate in tlds or wildcard_candidate in tlds):
            return ".".join(url_elements[i - 1:])
            # returns "abcde.co.uk"
    raise ValueError("Domain not in global list of TLDs")


def tokenize_plain_text(data):
    # no dot in splitters!
    splitters = ' /\'",;\t\n=(){}[]|\\`!@#$%^&*+'
    data = [data, ]
    for s in list(splitters):
        data = [x.strip() for d in data for x in d.split(s)]
    # remove empty
    data = filter(lambda x: len(x) > 0, data)
    # remove edge dots
    data = [d.strip('.').lower() for d in data]
    # remove those that do not have dots
    data = filter(lambda x: '.' in x, data)
    #print 'tokens: ', data
    return data


def check_pipe(files, field=-1, extract=False, splitter=", ", quiet=False):
    def check_single_candidate(line):
        url = line.split(splitter)[field] if field > -1 else line
        try:
            if field > -1:
                print >> sys.stdout, splitter.join(line[:field - 1] + get_domain(url, tlds) + line[field:])
            else:
                print >> sys.stdout, get_domain(url, tlds)
        except ValueError:
            if not quiet:
                print >> sys.stdout, 'Wrong url "%s" on line %d' % (url.strip(), counter)
    """Reads URLS from stdin one per line"""
    if field > -1 and not extract:
        print >> sys.stderr, 'Cannot use -f and -x options simultaneously!'
        sys.exit()

    tlds = get_tlds()
    counter = 0
    for line in fileinput.input(files):
        counter += 1
        if extract:
            [check_single_candidate(token) for token in tokenize_plain_text(line)]
        else:
            check_single_candidate(line)

if __name__ == "__main__":
    parser = OptionParser()
    parser.add_option("-f", "--field",  dest="field", default=-1,
                    help="Field number which should be convert inplace (starting 0), by default assume the data is single-columned",
                    metavar="field")
    parser.add_option("-s", "--splitter",  dest="splitter", default=", ",
                    help="Column splitter (use with -f option)",
                    metavar="splitter")
    parser.add_option("-x", "--extract",
                    action="store_true", dest="extract", default=False,
                    help="Extract domains from plain text")
    parser.add_option("-q", "--quiet",
                    action="store_true", dest="quiet", default=False,
                    help="Do not pring error messages")
    (options, files) = parser.parse_args()

    check_pipe(files, field=options.field, extract=options.extract,
        splitter=options.splitter, quiet=options.quiet)
